services:
  vllm:
    image: my-modelscope
    volumes:
      - .:/workspace
    working_dir: /workspace
    environment:
      - CUDA_VISIBLE_DEVICES=7
      - HF_TOKEN=${HF_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      swift deploy
      --use_hf true
      --infer_backend vllm
      --model meta-llama/Llama-3.1-8B-Instruct
      --adapters /workspace/output/llama31_8b_scam_real_sft_v4/v0-20260117-075831/checkpoint-108
      --served_model_name scam-8b-sft
      --max_new_tokens 1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 600s

  app:
    image: my-modelscope
    volumes:
      - .:/workspace
    working_dir: /workspace
    ports:
      - "9000:9000"
    depends_on:
      vllm:
        condition: service_healthy
    command: uvicorn app:app --host 0.0.0.0 --port 9000
